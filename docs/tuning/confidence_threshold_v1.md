# V1 Analysis Confidence Threshold Tuning\n\n**Related EPIC:** INTEGRITY-V1-001\n\n## Overview\n\nThis document outlines the approach and considerations for tuning the `MIN_CONFIDENCE_FOR_INSIGHTFUL_RESPONSE` threshold used in the V1 Integrity & Control Layer.\n\n## V1 Calculation Factors\n\nThe initial V1 confidence score (`calculateAnalysisConfidenceV1`) is based on a weighted combination of:\n\n1.  **Primary Appraisal Confidence (`pInstance.pAppraisalConfidence`):** Confidence score from the perception appraisal step.\n2.  **Average MHH Variable Confidence (`avgMhhConfidence`):** Average confidence across the inferred MHH rule variables (`source`, `perspective`, `timeframe`, `acceptanceState`).\n3.  **VAD Consistency Score (`vadConsistencyScore`):** Currently derived directly from the `vad.confidence` output of the VAD model (higher VAD confidence implies higher consistency).\n\n**Initial Weights (Subject to Tuning):**\n\n*   `pAppraisalConfidence`: 0.4\n*   `avgMhhConfidence`: 0.3\n*   `vadConsistencyScore`: 0.3\n\n## Initial Threshold Value\n\n*   **`PCE_CONFIDENCE_THRESHOLD` Default:** 0.6\n\n## Tuning Strategy (Post-V1 / Beta)\n\nThe initial threshold of 0.6 is an estimate.\ Tuning will require:\n\n1.  **Data Collection:** Logging the calculated `analysisConfidence` scores for all interactions during internal testing and Beta.\n2.  **Distribution Analysis:** Analyzing the distribution of confidence scores across different interaction types and user utterances.\n3.  **User Feedback Correlation:** Correlating confidence scores with user feedback from the \"Does this fit?\" mechanism (`analysis_feedback` table).\n    *   Do lower confidence scores correlate with more \"Not Quite\" responses?\n    *   Do higher confidence scores correlate with more \"Yes\" responses?\n4.  **Qualitative Review:** Manually reviewing interactions where the fallback occurred vs. where insightful responses were generated near the threshold boundary.\n5.  **Iterative Adjustment:** Adjusting the `PCE_CONFIDENCE_THRESHOLD` environment variable based on the findings to optimize the balance between providing helpful insights and avoiding potentially inaccurate reflections (epistemic humility).\n\n## Open Questions / Future Considerations\n\n*   Should the weighting factors themselves be adjusted?\n*   Should other factors from `EWEFAnalysisOutput` be incorporated (e.g., state confidence, EP activation strength)?\n*   How does the VAD consistency approximation (using `vad.confidence`) perform? Should we revert to a distance-based metric if VAD outputs change?\n 